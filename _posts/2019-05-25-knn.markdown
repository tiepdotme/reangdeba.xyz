---
layout: post
comments: true
title: "Understanding k-Nearest Neighbours in Python & Numpy with Applications"
excerpt: "I'll explain the inner-workings of the k-NN algorithm in depth, explore some math and the intuition behind it, touch upon a few applications and implement the algorithm from scratch using Python and Numpy."
date: 2019-05-26 00:00:00
mathjax: true
---
<!-- complete setup and code part -->
This tutorial was contributed by [Debashish Reang]("https://reangdeba.github.io/"). Feel free to get in touch for contributions, and suggestions.

<div class="imgcap">
    <img src="/assets/images/knn.png">
    <div class="thecap">
        <p>k-nearest neigbours with k = 6. <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">Image courtesy</a></p>
    </div>
</div>

This tutorial is aimed at introducing k-nearest neighbours (k-NN) to you. It is not used in practice now, but it can be a powerful tool in helping build the intuition and perspective to learning other models and algorithms. It involves in-depth explanation of the inner-workings of the algorithm, its applications (or at least historically), some math and intuition, and most importantly the code implemented from scratch. Finally, we would explore the limitations, and ways to fine tune the algorithm.

For the full code used on this page, visit [my GitHub repo](https://github.com/reangdeba/blog-code/tree/master/knn).

## Table of contents
- [Assumptions](#assume)
- [Introduction](#intro)
- [Nearest Neighbor](#nb)
- [What is k-NN?](#knn)
- [More on K](#k)
- [Preparing to cook](#code)
- [Limitations, Improvements](#pcs)
- [Summary](#summary)

<a name='assume'></a>

## Assumptions
This tutorial assumes that you are familiar with a few terms, concepts, and paradigms of machine learning. Supervised learning, dimensions, test set, training set, validation set etc. should sound familiar to you. It also assumes that you are proficient enough to understand and write code in Python. You should be able to make a few interpretations looking at plots, and code blocks. And most importantly the will to learn, and patience to read to the end is a must! I hope you enjoy the tutorial.

<a name='intro'></a>

## Introduction
k-NN is a classifier algorithm where the learning is based on how similar a datapoint is to others. It is one of the many supervised learning algorithms used in data mining and machine Learning. It is a non-parametric algorithm which means that there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. You can intuitively think of this algorithm as 'learning by analogy' that is, it looks at the dataset, remembers it, and then try to draw analogy in the data features.

The entire tutorial is motivated by the task of image classification, and I would use the problem of image classification to explain the concepts.
<a name='nb'></a>

## Nearest Neighbor

```python
class nearestNeighbor(object):
    def __init__(self):
        pass

    def train(images, label):
        # magic here!
        return model

    def predict(model, test_images):
        # use the model learned to predict labels on unseen images
        return test_labels
```

<div class="imgcap">
    <img src="/assets/images/knn_horse.png">
    <div class="thecap">
        <p>Look at the image above. The nearest neighbor classifier would predict the horse to be a car. Here's the idea, the classifier predicts the closest label that it thinks is the correct based on some metric like distance. Read on to find more.</p>
    </div>
</div>

Before we get into the details and the code of k-NN, it makes sense here to understand what a 'nearest neighbor' is. Here's the idea: so, you have the training data which is labeled. Then given a test image you want to predict the label of that image. An obvious way to do that is to look at the images that you have in training set and see to which image the given image is the most similar to. **Now the question is: how to quantify that similarity?**. There is no one correct or obvious answer to this one. There are many choices and approaches, but the one that this tutorial will use is the distance metric, i.e., how far is one image from the other. Now, the distance metric itself can have many formulations, we will look into L1 & L2 distances here.

**The distance metric.** There are many choices for the distance metric. L1, L2 distances are the common and simple ones.

$$d_1 (I_1, I_2) = \sum_{p} \left| I^p_1 - I^p_2 \right|$$

$$d_2 (I_1, I_2) = \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}$$

From the above image you can see that the nearest neighbour classifier wrongly classified the test image. It is strange to only use the label of the nearest image when making predictions. Instead, a reasonable approach would be to take a majority vote among the k-many neaighbours and make prediction. This intuitively means that even if the closest neighbor gives the wrong prediction, when you consider the k-closest images and take vote of their labels, chances are you would get fairly more accurate label prediction than just using one.

<a name='knn'></a>

## What is k-NN?
**Notations.** We will use $$x$$ to denote a *feature* (aka. predictor, attribute) and $$y$$ to denote the *target* (aka. label, class) we are trying to predict.

k-NN falls in the **supervised learning** family of algorithms. It is one of the simplest algorithms in Machine Learning. Informally, this means that we are given a labelled dataset consiting of training observations $$(x,y)$$ and would like to capture the relationship between $$x$$ and $$y$$. More formally, our goal is to learn a function $$h:X\rightarrow{Y}$$ so that given an unseen observation $$x$$, the function $$h(x)$$ can confidently predict the corresponding output $$y$$.

k-NN is a type of non-parametric, instance-based learning, or lazy learning algorithm.
* **Non-parametric.** It makes no explicit assumptions about the functional form of $$h$$, avoiding the dangers of mismodeling the underlying distribution of the data. For example, suppose our data is highly non-Gaussian but the learning model we choose assumes a Gaussian form. In that case, our algorithm would make extremely poor predictions.
* **Instance-based.** The function is only approximated locally and all computation is deferred until classification.

> k-NN is instance-based, non-parametric learning algorithm.
The k-NN is similar to NN classifier except that it takes majority vote of k nearest neighbors for prediction rather than solely using the nearest neighbor.

If you paid attention, you must have noticed that the algorithm computes **during** the time of classification (testing). It also requires that we remember the training data, so the amount of memory required would be substantial for large datasets. This is not desirable, since we can do with our classifier taking time during training but at the time of prediction we want it to predict fast.

> Minimal training but expensive testing.

<a name='k'></a>

## More on K
The k-nearest neighbor classifier requires a setting for k. But what number works best? At this point, you’re probably wondering how to pick the variable k and what its effects are on your classifier. Well, like most machine learning algorithms, the k in k-NN is a **hyperparameter** that you, as a designer, must pick in order to get the best possible fit for the data set. Intuitively, you can think of k as controlling the shape of the decision boundary between classes.

<div class='imgcap'>
    <img src='/assets/images/knn_large.jpeg'>
    <div class='thecap'>
        <p>When k is small, we are restraining the region of a given prediction and forcing our classifier to be “more blind” to the overall distribution. A small value for k provides the most flexible fit, which will have low bias but high variance. Our decision boundary will be more jagged. On the other hand, a higher k averages more voters in each prediction and hence is more resilient to outliers. Intuitively, it smoothes out the boundaries. Larger values of k will have lower variance but increased bias.</p>
    </div>
</div>
(If you want to learn more about the bias-variance tradeoff, check out [Scott Roe’s Blog post](http://scott.fortmann-roe.com/docs/BiasVariance.html). You can mess around with the value of k and watch the decision boundary change!)

**Okay, but how to find k?**
Luckily, there is a correct way of tuning the hyperparameter. The idea is to split our training set in two: a slightly smaller training set, and what we call a validation set. In other words: given a training set, we break it up into two parts- a new training set and a new test set (which we call *validation set*), and then run our classifier on this new test set. By the end of this procedure, we could plot a graph that shows which values of k work best. We would then stick with this value and evaluate once on the actual test set.

> Split your training set into training set and a validation set. Use validation set to tune all hyperparameters. At the end run a single time on the test set and report performance.

**A more sophisticated technique.**<br>
**Cross-validation.** In cases where the size of your training data (and therefore also the validation data) might be small, people sometimes use a more sophisticated technique for hyperparameter tuning called cross-validation. For example, in 5-fold cross-validation, we would split the training data into 5 equal folds (parts), use 4 of them for training, and 1 for validation. We would then iterate over which fold is the validation fold, evaluate the performance, and finally average the performance across the different folds.

<div class='imgcap'>
    <img src='/assets/images/cvplot.png'>
    <div class='thecap'>
        <p>Example of a 5-fold cross-validation run for the parameter k. For each value of k we train on 4 folds and evaluate on the 5th. Hence, for each k we receive 5 accuracies on the validation fold (accuracy is the y-axis, each result is a point). The trend line is drawn through the average of the results for each k and the error bars indicate the standard deviation. Note that in this particular case, the cross-validation suggests that a value of about k = 7 works best on this particular dataset (corresponding to the peak in the plot). If we used more than 5 folds, we might expect to see a smoother (i.e. less noisy) curve.</p>
        <a href="http://cs231n.github.io/classification/">Credit: cs231n notes.</a>
    </div>
</div>


<a name='code'></a>

## Preparing to cook
<div class='imgcap'>
    <img src='/assets/images/cifar10.png'>
    <div class='thecap'>
        <a href="https://www.cs.toronto.edu/~kriz/cifar.html">Credit: CIFAR-10.</a>
    </div>
</div>

Let's implement what we learned so far in code. The best way to understand how and why something works is to implement and play around with the thing yourself! We will write the k-NN algorithm from scratch, and use it to predict labels of [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). For that you need to download the dataset, do preprocessing of the data before you can use it as input to the algorithm. The next section describes the setup.

<a name='dataset'></a>
### Setting up
This section has instructions for setting up things on your machine, in case you want to run the code and see. If you are only interested in the algorithm, feel free to skip to [the algorithm](#algo).

**Installing Anaconda:** If you decide to work locally, we recommend using the free [Anaconda Python distribution](https://www.anaconda.com/distribution/), which provides an easy way for you to handle package dependencies. Please be sure to download the Python 3 version, which currently installs Python 3.7.

**Anaconda virtual environment:** Once you have Anaconda installed, it makes sense to create a virtual environment for this task. If you choose not to use a virtual environment, it is up to you to make sure that all dependencies for the code are installed globally on your machine.<br> To set up a virtual environment, run (in a terminal) `conda create -n tutorial python=3.7 anaconda` to create an environment called `tutorial`.

Then, to activate and enter the environment, run `source activate tutorial`. To exit, you can simply close the window, or run `source deactivate tutorial`.<br>
You may refer to [this page](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html) for more detailed instructions on managing virtual environments with Anaconda.

Download the starter code from [here](https://res.cloudinary.com/reangdeba/raw/upload/v1558873071/knn.zip). Extract, and `cd path/to/directory`. Assuming that you followed the setup instruction above, type `jupyter notebook` in the terminal while you're at `knn directory`. This should start the notebook server. Open the `knn.ipynb` notebook. Further instructions are written there. Feel free to play around with the code, if interested let me know of the interpretations you made and the challenges you faced.<br>Make sure you `cd path/to/datasets/directory` and run `./get_datasets.sh` before running the cells in the notebook.


<a name='algo'></a>

### The algorithm
**The classifier algorithm.** Like almost every other machine learning algorithm this algorithm also has two main parts namely: `train` and `predict`. The algorithm also needs **distance metric** for prediction, so `compute_distances` is also implemented. The code block below looks longish one (mainly because of the comments). But, all that the algorithm is doing is train the model, compute distances and predict labels!

```python
class KNearestNeighbor(object):
    """ a kNN classifier with L2 distance """
    def __init__(self):
        pass

    def train(self, X, y):
        """
        Train the classifier. For k-nearest neighbors this is just
        memorizing the training data.

        Inputs:
        - X: A numpy array of shape (num_train, D) containing the training data
          consisting of num_train samples each of dimension D.
        - y: A numpy array of shape (N,) containing the training labels, where
             y[i] is the label for X[i].
        """
        self.X_train = X
        self.y_train = y

    def compute_distances(self, X):
        """
        Compute the distance between each test point in X and each training point
        in self.X_train.

        Inputs:
        - X: A numpy array of shape (num_test, D) containing test data.

        Returns:
        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]
          is the Euclidean distance between the ith test point and the jth training
          point.
        """
        num_test = X.shape[0]
        num_train = self.X_train.shape[0]
        dists = np.zeros((num_test, num_train))

        a = np.sqrt(-2 * np.dot(X, self.X_train.T)
        b = np.sum(self.X_train**2, axis = 1)
        c = np.sum(X**2, axis = 1)[:, np.newaxis])

        dists =  a + b + c

        return dists

    def predict_labels(self, dists, k=1):
        """
        Given a matrix of distances between test points and training points,
        predict a label for each test point.

        Inputs:
        - dists: A numpy array of shape (num_test, num_train) where dists[i, j]
          gives the distance betwen the ith test point and the jth training point.

        Returns:
        - y: A numpy array of shape (num_test,) containing predicted labels for the
          test data, where y[i] is the predicted label for the test point X[i].
        """
        num_test = dists.shape[0]
        y_pred = np.zeros(num_test)
        for i in range(num_test):
            # A list of length k storing the labels of the k nearest neighbors to
            # the ith test point.
            closest_y = []
            
            # find knns
            nbrs = np.argsort(dists[i, :])[:k]
            for nbr in nbrs:
                closest_y.append(self.y_train[nbr])
            
            # credits: geeksforgeeks
            # a neat implementation for choosing 
            # the label that occurs maximum number of times 
            # take majority vote
            def most_frequent(list_): 
                dict = {} 
                count, itm = 0, '' 
                for item in reversed(list_): 
                    dict[item] = dict.get(item, 0) + 1
                    if dict[item] >= count : 
                        count, itm = dict[item], item 
                return(itm)
            
            y_pred[i] = most_frequent(closest_y)

        return y_pred
```

<a name='pcs'></a>

## Limitations, Improvements
### Pros
* Simple to understand and easy to implement as you can see.

* Many algorithms are hard-coded for binary classification. But, k-NN works just as fine with multiclass datasets as you saw if you played with the code yourself.

* k-NN works pretty okay even in settings where the data is "unusual" since it does not look into the underlying distribution of the data. It makes no explicit assumptions about the functional form of the data.

### Cons
* The k-NN algorithm is computationally expensive during testing phase which is impractical in industry settings. **Note** contrast this to sophisticated Neural Networks which has lengthy training phase but only take miliseconds during prediction.

* k-NN can suffer from **skewed class distributions**. If a label say, is very frequent in the dataset then the classifier can wrongly predict test labels more often than not.

* The accuracy of k-NN can be severely degraded with high-dimension data because there is little difference between the nearest and farthest neighbor (*curse of dimensionality*).

### Improvements
There are many ways in which the k-NN algorithm can be improved.

* A simple and effective way to **remedy skewed class distributions** is by implementing weighed voting. The class of each of the k neighbors is multiplied by a weight proportional to the inverse of the distance from that point to the given test point. This ensures that nearer neighbors contribute more to the final vote than the more distant ones.

* **Changing the distance metric** for different applications may help improve the accuracy of the algorithm. (i.e. Hamming distance for text classification)

* **Rescaling your data** makes the distance metric more meaningful. For instance, given 2 features height and weight, an observation such as x=[180,70] will clearly skew the distance metric in favor of height. One way of fixing this is by column-wise subtracting the mean and dividing by the standard deviation.

* **Dimensionality reduction techniques** like PCA ([read Andrew's note](http://cs229.stanford) on this) should be executed prior to appplying k-NN and help make the distance metric more meaningful.

* **Approximate Nearest Neighbor techniques**. A good implementation by [Facebook AI Research](https://github.com/facebookresearch/faiss).

<a name='summary'></a>

## Summary
In this tutorial, you learned about the k-Nearest Neighbor algorithm, how it works and how it can be applied in a classification setting. We also implemented the algorithm in Python and Numpy from scratch in such a way that we understand the inner-workings of the algorithm. We even used matplotlib to create visualizations to further understand our data. Finally, we explored the pros and cons of k-NN and the many improvements that can be made to adapt it to different project settings.

If you want to practice some more with the algorithm, try and run it on the [Breast Cancer Wisconsin dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)) which you can find in the [UC Irvine Machine Learning repository](https://archive.ics.uci.edu/ml/index.php). You’ll need to preprocess the data carefully this time. Do it once with scikit-learn’s algorithm and a second time with our version of the code but try adding the weighted distance implementation.

## Further readings
If are interested in understanding more about how the algoritm works, and want to learn more of ML, ML techniques etc. you can check the following resources.

* [Stanford's CS229](http://cs229.stanford.edu/), used to be taught by Andrew Ng.
* [Hugo Larochelle's](http://info.usherbrooke.ca/hlarochelle/cours/ift725_A2014/description.html) nice, but fairly advanced class on Machine Learning. Note that the page is in French but the slides and videos are in English.
* [A Few Useful Things to Know about Machine Learning](http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf). An interesting read.

## References
### Notes
* Stanford's [cs231n notes on k-NN and classification](http://cs231n.github.io/classification/#knn)

* [Kevin Zakka's guide on k-NN](https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/)

Thank you for reading my guide, and I hope it helped you in theory & practice! Feel free to drop a line in case of doubts, appreciation, suggestions etc.<br>
<!-- Thanks also to Ankit Meel for reading the guide and suggesting improvements. -->